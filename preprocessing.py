import numpy as np
import pandas as pd
import torch
import mysql.connector
import os
from dotenv import load_dotenv
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler, RobustScaler

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# MySQL ì—°ê²° ì„¤ì •
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")

def create_connection():
    """ MySQL ì—°ê²°ì„ ìƒì„± """
    return mysql.connector.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )

def fetch_latest_ocean_data():
    """ MySQLì—ì„œ ìµœì‹  ocean ë°ì´í„° ê°€ì ¸ì˜¤ê¸° """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        query = """
        SELECT wind_speed, air_temperature, humidity, air_pressure, water_temperature, salinity
        FROM oceandata
        ORDER BY datetime DESC
        LIMIT 1
        """
        cursor.execute(query)
        result = cursor.fetchone()

        cursor.close()
        conn.close()

        if result:
            return list(result.values())  # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        else:
            return None
    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None


def fetch_latest_ais_data():
    """ ê° ì„ ë°•(MMSI)ë³„ ìµœì‹  2ê°œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ ìœ„ì¹˜ + ì´ì „ ìœ„ì¹˜) """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        # ê° ì„ ë°•(MMSI)ë³„ ìµœì‹  2ê°œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        query = """
        WITH RankedAIS AS (
            SELECT *, 
                ROW_NUMBER() OVER (PARTITION BY mmsi ORDER BY timestamp DESC) AS rn
            FROM aisdata
        )
        SELECT * FROM RankedAIS WHERE rn <= 2;
        """
        cursor.execute(query)
        results = cursor.fetchall()

        cursor.close()
        conn.close()

        if not results:
            return None  # ë°ì´í„° ì—†ìŒ

        # ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results)

        # MMSIë³„ë¡œ ìµœì‹  ë°ì´í„° 2ê°œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if df["mmsi"].nunique() == 0 or df.shape[0] < 2:
            return None  # ê° ì„ ë°•ë³„ë¡œ 2ê°œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê±°ë¦¬ ê³„ì‚° ë¶ˆê°€

        return df

    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None


def haversine_vectorized(lat1, lon1, lat2, lon2):
    """ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚° """
    R = 6371  # ì§€êµ¬ ë°˜ê²½ (km)
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1

    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

    return R * c  # km ë‹¨ìœ„ ê±°ë¦¬ ë°˜í™˜

def preprocess():
    """ ê° ì„ ë°•(MMSI)ë³„ë¡œ ì´ë™ ê±°ë¦¬, ì‹ í˜¸ ì†Œì‹¤ì„ ê³„ì‚°í•˜ê³  ì „ì²˜ë¦¬ """
    df = fetch_latest_ais_data()

    if df is None:
        print("No valid data available. Skipping processing.")
        return None

    # ì‹œê°„ ì°¨ì´ ê³„ì‚°
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = df.sort_values(["mmsi", "timestamp"])  # MMSIë³„ ì •ë ¬
    df["time_diff"] = df.groupby("mmsi")["timestamp"].diff().dt.total_seconds()

    # ì‹ í˜¸ ì†Œì‹¤ ì—¬ë¶€ (5ë¶„(300ì´ˆ) ì´ìƒ ì‹ í˜¸ ì—†ìŒ)
    df["signal_loss"] = df["time_diff"] > 300

    # ì´ë™ ê±°ë¦¬ ê³„ì‚°
    df["prev_lat"] = df.groupby("mmsi")["lat"].shift(1)
    df["prev_lon"] = df.groupby("mmsi")["lon"].shift(1)

    df["distance"] = df.apply(lambda row: haversine_vectorized(row["prev_lat"], row["prev_lon"], row["lat"], row["lon"]) 
                              if not pd.isna(row["prev_lat"]) else np.nan, axis=1)

    # ì´ë™ ê±°ë¦¬ ì´ìƒì¹˜ í•„í„°ë§ (5km ì´ìƒ ì´ë™í•œ ê²½ìš° + ì‹ í˜¸ ì†Œì‹¤ ì—†ìœ¼ë©´ ì œê±°)
    df = df[~((df["distance"] > 5) & (~df["signal_loss"]))]

    # í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ
    df = df.drop(columns=["prev_lat", "prev_lon", "signal_loss", "time_diff"])

    # ì„ í˜• ë³´ê°„ ì ìš© (speed, turn, course, heading)
    cols_to_interpolate = ["speed", "turn", "course", "heading"]
    df[cols_to_interpolate] = df[cols_to_interpolate].interpolate(method="linear", limit_direction="both")

    # ìŠ¤ì¼€ì¼ë§ ì ìš©
    scale_columns = ["turn", "speed", "accuracy", "course", "heading", "distance"]
    scaler = ColumnTransformer([
        ("robust", RobustScaler(), ["turn"]),
        ("minmax", MinMaxScaler(), scale_columns)
    ])
    normalized_data = scaler.fit_transform(df[scale_columns])

    # PyTorch Tensor ë³€í™˜ (ê° MMSIë³„ ë°ì´í„° ë°˜í™˜)
    tensors = {mmsi: torch.tensor(normalized_data[idx], dtype=torch.float32)
               for idx, mmsi in enumerate(df["mmsi"].values)}

    return tensors  # MMSIë³„ í…ì„œ ë°˜í™˜

if __name__ == "__main__":
    # ğŸ”¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    processed_data = preprocess()
    print("ì‹¤ì‹œê°„ ì „ì²˜ë¦¬ëœ ë°ì´í„°:", processed_data)


