#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd

data = pd.read_csv("../document/merged_data.csv")
data.head()


# In[2]:


data = data.drop(columns=['timestamp'])  # 'timestamp 'ì„ ì‚­ì œ

# ê²°ê³¼ ì €ì¥
data.to_csv('../document/drop_merged_data.csv', index=False)  # ìˆ˜ì •ëœ ë°ì´í„°ë¥¼ ì €ì¥


# In[3]:


# NaN ê°’ í™•ì¸
print(data.isna().sum())


# In[4]:


# NaN ê°’ì´ ìˆëŠ” í–‰ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
first_nan_index = data['datetime'].isna().idxmax()  # NaN ê°’ì´ ì²˜ìŒ ë‚˜íƒ€ë‚˜ëŠ” ì¸ë±ìŠ¤
print(f"'datetime' ì—´ì—ì„œ NaN ê°’ì´ ì²˜ìŒ ë‚˜íƒ€ë‚˜ëŠ” í–‰ì˜ ì¸ë±ìŠ¤: {first_nan_index}")

# NaN ê°’ì´ ì‹œì‘í•˜ëŠ” í–‰ ë°ì´í„° í™•ì¸
print(data.iloc[first_nan_index])


# In[5]:


# íŠ¹ì • í–‰(1172700~1172800)ì˜ 'created_at'ê³¼ 'datetime' ì—´ë§Œ ì„ íƒ
subset = data.iloc[1791300:1797959][['created_at', 'datetime']]
print(subset)


# In[6]:


# íŠ¹ì • í–‰ ë²ˆí˜¸ ì´í›„ ë°ì´í„° ì‚­ì œ
row_to_drop_from = 1791300  # ì‚­ì œë¥¼ ì‹œì‘í•  í–‰ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
data_dropped = data.iloc[:row_to_drop_from]

# mmsi ê°’ì´ 0ì¸ ë°ì´í„° ì œê±°
data_cleaned = data_dropped[data_dropped['mmsi'] != 0]

# ê²°ê³¼ í™•ì¸
print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {data_cleaned.shape}")


# ê²°ê³¼ ì €ì¥
data_cleaned.to_csv('../document/data_after_dropping.csv', index=False, encoding='utf-8-sig')

print("ìµœì¢… í´ë¦° ë°ì´í„°ê°€ '../document/data_after_dropping.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[8]:


df = pd.read_csv("../document/data_after_dropping.csv")
df.head()


# In[9]:


print(df.isna().sum())


# In[10]:


import pandas as pd

# ì›ë³¸ ë°ì´í„° ë¡œë“œ
file_path = '../document/data_after_dropping.csv'
data = pd.read_csv(file_path)

# ì‹œê°„ ë°ì´í„° ë³€í™˜
data['created_at'] = pd.to_datetime(data['created_at'])

# ì‹œê°„ ìˆœ ì •ë ¬
data = data.sort_values(['mmsi', 'created_at'])

# MMSIë³„ ì„ í˜• ë³´ê°„ ì ìš©
data['lat'] = data.groupby('mmsi')['lat'].transform(lambda x: x.interpolate(method='linear'))
data['lon'] = data.groupby('mmsi')['lon'].transform(lambda x: x.interpolate(method='linear'))

# ìµœì¢… ë°ì´í„° ì €ì¥
data.to_csv('../document/final_cleaned_datalinear.csv', index=False, encoding='utf-8-sig')

print(f"ìµœì¢… ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: {len(data)}")


# In[11]:


import pandas as pd

# ê²½ë¡œ ì„¤ì • (ì›ë³¸ ë°ì´í„°ì™€ ìµœì¢… ë°ì´í„°)
original_file_path = '../document/data_after_dropping.csv'
final_cleaned_file_path = '../document//final_cleaned_datalinear.csv'

# ë°ì´í„° ë¡œë“œ
original_data = pd.read_csv(original_file_path)
final_cleaned_data = pd.read_csv(final_cleaned_file_path)

# ë°ì´í„° í¬ê¸° ë¹„êµ
original_count = len(original_data)
final_cleaned_count = len(final_cleaned_data)

# ì—´ ì´ë¦„ ë¹„êµ
original_columns = original_data.columns.tolist()
final_cleaned_columns = final_cleaned_data.columns.tolist()

# ê²°ê³¼ ì¶œë ¥
print(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {original_count}")
print(f"ìµœì¢… í´ë¦° ë°ì´í„° í–‰ ìˆ˜: {final_cleaned_count}")
print(f"ì›ë³¸ ë°ì´í„° ì—´ ì´ë¦„: {original_columns}")
print(f"ìµœì¢… í´ë¦° ë°ì´í„° ì—´ ì´ë¦„: {final_cleaned_columns}")

# ìµœì¢… í´ë¦° ë°ì´í„°ì— ì—†ëŠ” í–‰ í™•ì¸
removed_rows = original_data[~original_data.set_index(['mmsi', 'created_at']).index.isin(final_cleaned_data.set_index(['mmsi', 'created_at']).index)]
print(f"ì œê±°ëœ ë°ì´í„° í–‰ ìˆ˜: {len(removed_rows)}")

# ì œê±°ëœ ë°ì´í„° í™•ì¸
print("ì œê±°ëœ ë°ì´í„° ì˜ˆì‹œ:")
print(removed_rows
      )


# In[13]:


# # ì´ latê³¼ lonì´ ê²°ì¸¡ì¹˜ì¸ í–‰ì˜ ê°¯ìˆ˜
# total_nan = result_df[(result_df['lat_is_nan'] & result_df['lon_is_nan'])]

# # ì§€ê¸ˆ, ì•ë’¤ë¡œ ë‹¤ ì—†ëŠ” í–‰ì˜ ê°¯ìˆ˜
# all_nan = result_df[
#     (result_df['lat_is_nan'] & result_df['lon_is_nan']) &
#     (~result_df['has_previous_lat'] & ~result_df['has_previous_lon']) &
#     (~result_df['has_next_lat'] & ~result_df['has_next_lon'])
# ]

# # ì§€ê¸ˆì€ ì—†ì§€ë§Œ ì•ë’¤ë¡œëŠ” ë‹¤ ìˆëŠ” í–‰ì˜ ê°¯ìˆ˜
# surrounded_by_data = result_df[
#     (result_df['lat_is_nan'] & result_df['lon_is_nan']) &
#     (result_df['has_previous_lat'] & result_df['has_previous_lon']) &
#     (result_df['has_next_lat'] & result_df['has_next_lon'])
# ]

# # ì•ì´ ì—†ê³  ë’¤ë§Œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
# only_next_data = result_df[
#     (result_df['lat_is_nan'] & result_df['lon_is_nan']) &
#     (~result_df['has_previous_lat'] & ~result_df['has_previous_lon']) &
#     (result_df['has_next_lat'] & result_df['has_next_lon'])
# ]

# # ì•ì€ ìˆê³  ë’¤ëŠ” ì—†ëŠ” ê²½ìš°
# only_previous_data = result_df[
#     (result_df['lat_is_nan'] & result_df['lon_is_nan']) &
#     (result_df['has_previous_lat'] & result_df['has_previous_lon']) &
#     (~result_df['has_next_lat'] & ~result_df['has_next_lon'])
# ]

# # ì§€ê¸ˆì€ ì—†ì§€ë§Œ ì• í˜¹ì€ ë’¤ì— ë°ì´í„°ê°€ ìˆëŠ” ê°¯ìˆ˜
# has_either_data = result_df[
#     (result_df['lat_is_nan'] & result_df['lon_is_nan']) &
#     ((result_df['has_previous_lat'] & result_df['has_previous_lon']) |
#      (result_df['has_next_lat'] & result_df['has_next_lon']))
# ]

# # ê²°ê³¼ ì¶œë ¥
# print(f"ì´ latê³¼ lonì´ ê²°ì¸¡ì¹˜ì¸ í–‰ì˜ ê°¯ìˆ˜: {len(total_nan)}")
# print(f"ì§€ê¸ˆ, ì•ë’¤ë¡œ ë‹¤ ì—†ëŠ” í–‰ì˜ ê°¯ìˆ˜: {len(all_nan)}")
# print(f"ì§€ê¸ˆì€ ì—†ì§€ë§Œ ì•ë’¤ë¡œëŠ” ë‹¤ ìˆëŠ” í–‰ì˜ ê°¯ìˆ˜: {len(surrounded_by_data)}")
# print(f"ì•ì´ ì—†ê³  ë’¤ë§Œ ë°ì´í„°ê°€ ìˆëŠ” í–‰ì˜ ê°¯ìˆ˜: {len(only_next_data)}")
# print(f"ì•ì€ ìˆê³  ë’¤ëŠ” ì—†ëŠ” í–‰ì˜ ê°¯ìˆ˜: {len(only_previous_data)}")
# print(f"ì§€ê¸ˆ ì—†ì§€ë§Œ ì• í˜¹ì€ ë’¤ì— ë°ì´í„°ê°€ ìˆëŠ” ê°¯ìˆ˜: {len(has_either_data)}")


# In[12]:


import pandas as pd

# ë°ì´í„° ë¡œë“œ
data = pd.read_csv('../document/final_cleaned_datalinear.csv')

# NaNì„ ìƒˆë¡œìš´ ë²”ì£¼ë¡œ ì¶”ê°€
data['status'] = data['status'].fillna('nan')  # NaN ê°’ì„ 'nan' ë¬¸ìì—´ë¡œ ë³€í™˜

# ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰
status_encoded = pd.get_dummies(data['status'], prefix='status')

# True/Falseë¥¼ 0/1ë¡œ ë³€í™˜
status_encoded = status_encoded.astype(int)

# ì›ë³¸ ë°ì´í„°ì™€ ì›-í•« ì¸ì½”ë”©ëœ ë°ì´í„° ë³‘í•©
data = pd.concat([data, status_encoded], axis=1)

# ê²°ê³¼ í™•ì¸
print(data.head())

# ê²°ê³¼ ì €ì¥
data.to_csv('../document/final_cleaned_data_with_nan_encodedlinear.csv', index=False, encoding='utf-8-sig')

print("ê²°ê³¼ ë°ì´í„°ê°€ 'final_cleaned_data_with_nan_encoded.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[13]:


# ëŒ€í•œë¯¼êµ­ ìœ„ë„ ë° ê²½ë„ ë²”ìœ„
lat_min, lat_max = 34.5, 35.5
lon_min, lon_max = 128.5, 130.0

# ëŒ€í•œë¯¼êµ­ ë²”ìœ„ ë‚´ ë°ì´í„° í•„í„°ë§
data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &
                    (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]

# ì´ìƒì¹˜ ë°ì´í„° í™•ì¸ (ëŒ€í•œë¯¼êµ­ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°)
outliers = data[~((data['lat'] >= lat_min) & (data['lat'] <= lat_max) &
                  (data['lon'] >= lon_min) & (data['lon'] <= lon_max))]
print("ì´ìƒì¹˜ ë°ì´í„°:\n", outliers)

# ê²°ê³¼ í™•ì¸
print(f"ëŒ€í•œë¯¼êµ­ ë²”ìœ„ ë‚´ ë°ì´í„° í¬ê¸°: {data.shape}")

data.to_csv('../document/cleaned_data_korealinear.csv', index=False, encoding='utf-8-sig')
print("í´ë¦° ë°ì´í„°ê°€ 'cleaned_data_korea.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[17]:


# status ê°’ì˜ ê³ ìœ  ê°’ ë° ë¶„í¬ í™•ì¸
print("status ê°’ ë¶„í¬:")
print(data['status'].value_counts(dropna=False))


# In[18]:


# ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ í™•ì¸
for column in data.columns:
    try:
        data[column].astype(float)
    except ValueError as e:
        print(f"ì—´ '{column}'ì— ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê°’ì´ ìˆìŠµë‹ˆë‹¤.")


# In[19]:


# ë¬¸ì œ ì—´ì—ì„œ ìˆ«ìê°€ ì•„ë‹Œ ê°’ í•„í„°ë§
print(data['salinity'][~data['salinity'].str.replace('.', '', 1).str.isdigit()])


# In[20]:


data['salinity'] = data['salinity'].fillna(method='ffill')  # ì•ì˜ ê°’ìœ¼ë¡œ ì±„ì›€
data['salinity'] = data['salinity'].fillna(method='bfill')  # ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ì›€


# In[21]:


print(data['salinity'].unique())


# In[22]:


import numpy as np

# '-' ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
data['salinity'] = data['salinity'].replace('-', np.nan).astype(float)
# ì•ì˜ ê°’ìœ¼ë¡œ ëŒ€ì²´
data['salinity'] = data['salinity'].fillna(method='ffill')

# ë’¤ì˜ ê°’ìœ¼ë¡œ ëŒ€ì²´
data['salinity'] = data['salinity'].fillna(method='bfill')


# In[23]:


# NaN ê°’ í™•ì¸
print(data['salinity'].isna().sum())

# salinity ê³ ìœ  ê°’ í™•ì¸
print(data['salinity'].unique())


# In[30]:


data['turn'] = data['turn'].interpolate(method='linear', limit_direction='both')
data['speed'] = data['speed'].interpolate(method='linear', limit_direction='both')
data['course'] = data['course'].interpolate(method='linear', limit_direction='both')
data['heading'] = data['heading'].interpolate(method='linear', limit_direction='both')
data['accuracy'] = data['accuracy'].interpolate(method='linear', limit_direction='both')


# In[31]:


print(data.isna().sum())


# In[46]:


import pandas as pd
import numpy as np

# Haversine ê³µì‹ ì‚¬ìš©í•˜ì—¬ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)
    
    # ìœ„ë„ ë° ê²½ë„ë¥¼ ë¼ë””ì•ˆìœ¼ë¡œ ë³€í™˜
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    
    # ì°¨ì´ ê³„ì‚°
    dlat = lat2 - lat1
    dlon = lon2 - lon1

    # Haversine ê³µì‹ ì ìš©
    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    distance = R * c  # km ë‹¨ìœ„ ê±°ë¦¬ ë°˜í™˜

    return distance

# ì‹œê°„ ë°ì´í„° ë³€í™˜ ë° ì •ë ¬
data['created_at'] = pd.to_datetime(data['created_at'])
data = data.sort_values(['mmsi', 'created_at'])

# Feature ë¦¬ìŠ¤íŠ¸
features = []

# ê°€ì†ë„ ê³„ì‚°
data['prev_speed'] = data.groupby('mmsi')['speed'].shift(1)
data['time_diff'] = data.groupby('mmsi')['created_at'].diff().dt.total_seconds()
data['acceleration'] = (data['speed'] - data['prev_speed']) / data['time_diff']
data['acceleration'].fillna(0, inplace=True)
features.append('acceleration')

# íšŒì „ìœ¨ ê³„ì‚°
data['prev_heading'] = data.groupby('mmsi')['heading'].shift(1)
data['heading_diff'] = (data['heading'] - data['prev_heading'] + 180) % 360 - 180
data['heading_diff'].fillna(0, inplace=True)
features.append('heading_diff')

# ê±°ë¦¬ ê³„ì‚°
data['prev_lat'] = data.groupby('mmsi')['lat'].shift(1)
data['prev_lon'] = data.groupby('mmsi')['lon'].shift(1)
data['distance_km'] = data.apply(lambda row: haversine(row['prev_lat'], row['prev_lon'], row['lat'], row['lon']) if pd.notnull(row['prev_lat']) else 0, axis=1)
features.append('distance_km')

# ì´ë™ í‰ê·  ë° ìœ„ì¹˜ ë³€í™”ëŸ‰ ê³„ì‚°
for window in [5, 10, 30]:  
    speed_feature = f'avg_speed_{window}steps'
    heading_feature = f'avg_heading_{window}steps'
    position_feature = f'position_change_{window}steps'

    data[speed_feature] = data.groupby('mmsi')['speed'].rolling(window).mean().reset_index(level=0, drop=True)
    data[heading_feature] = data.groupby('mmsi')['heading'].rolling(window).mean().reset_index(level=0, drop=True)

    data[f'prev_lat_{window}'] = data.groupby('mmsi')['lat'].shift(window)
    data[f'prev_lon_{window}'] = data.groupby('mmsi')['lon'].shift(window)
    data[position_feature] = data.apply(lambda row: haversine(row['lat'], row['lon'], row[f'prev_lat_{window}'], row[f'prev_lon_{window}']) if pd.notnull(row[f'prev_lat_{window}']) else 0, axis=1)

    features.extend([speed_feature, heading_feature, position_feature])

# ê²°ì¸¡ê°’ ì²˜ë¦¬
for feature in features:
    data[feature].fillna(method='ffill', inplace=True)
    data[feature].fillna(method='bfill', inplace=True)
    data[feature].fillna(0, inplace=True)

# íŒŒì¼ ì €ì¥ ê²½ë¡œ
save_path = "../document/feature_extracted_data.csv"

# CSVë¡œ ì €ì¥
data.to_csv(save_path, index=False, encoding="utf-8-sig")



# In[47]:





# In[34]:


print("ì¶”ì¶œëœ Features:", features)


# In[35]:


# ìŒìˆ˜ ê°’ í™•ì¸ ëŒ€ìƒ ì—´
columns_to_check = [
    'turn', 'speed', 'accuracy', 'course', 'heading', 
    'wind_direct', 'wind_speed', 'surface_curr_drc', 'surface_curr_speed', 
    'air_temperature', 'water_temperature', 'air_pressure', 'humidity', 'salinity',
    'acceleration', 'heading_diff', 'distance_km', 'avg_speed_5steps', 'avg_heading_5steps', 'position_change_5steps', 'avg_speed_10steps', 'avg_heading_10steps', 'position_change_10steps', 'avg_speed_30steps', 'avg_heading_30steps', 'position_change_30steps'
]

# ì„ íƒí•œ ì—´ì—ì„œ ìŒìˆ˜ ê°’ í™•ì¸
negative_values = (data[columns_to_check] < 0).sum()

# ìŒìˆ˜ ê°’ì´ ìˆëŠ” ì—´ í•„í„°ë§
negative_columns = negative_values[negative_values > 0]

# ê²°ê³¼ ì¶œë ¥
if len(negative_columns) > 0:
    print("ìŒìˆ˜ ê°’ì´ í¬í•¨ëœ ì—´ê³¼ ê·¸ ê°œìˆ˜:")
    print(negative_columns)
else:
    print("ì„ íƒí•œ ì—´ì— ìŒìˆ˜ ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")


# In[39]:


from sklearn.preprocessing import MinMaxScaler, RobustScaler

# ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ì—´
scale_columns = [
    'turn', 'speed', 'accuracy', 'course', 'heading', 
    'wind_direct', 'wind_speed', 'surface_curr_drc', 'surface_curr_speed', 
    'air_temperature', 'water_temperature', 'air_pressure', 'humidity', 'salinity',
    'acceleration', 'heading_diff', 'distance_km', 'avg_speed_5steps', 'avg_heading_5steps', 'position_change_5steps', 'avg_speed_10steps', 'avg_heading_10steps', 'position_change_10steps', 'avg_speed_30steps', 'avg_heading_30steps', 'position_change_30steps'
]

# MinMaxScalerì™€ RobustScaler ì´ˆê¸°í™”
scaler_minmax = MinMaxScaler()
scaler_robust = RobustScaler()

## ì›ë³¸ ë°ì´í„° ìœ ì§€
data_scaled = data.copy()

# RobustScaler ì ìš©í•  ì»¬ëŸ¼ ì§€ì •
robust_columns = ['turn', 'acceleration', 'heading_diff']

# inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
data[robust_columns] = data[robust_columns].replace([np.inf, -np.inf], np.nan)

# NaNì„ ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê³ , ê·¸ë˜ë„ NaNì´ë©´ 0ìœ¼ë¡œ ëŒ€ì²´
for col in robust_columns:
    data[col].fillna(method='ffill', inplace=True)  # ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    data[col].fillna(method='bfill', inplace=True)  # ì´í›„ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    data[col].fillna(0, inplace=True)  # ìµœì¢…ì ìœ¼ë¡œ 0ìœ¼ë¡œ ëŒ€ì²´

print("Infinite values handled successfully! ğŸš€")

# RobustScaler & MinMaxScaler ì •ì˜
scaler_robust = RobustScaler()
scaler_minmax = MinMaxScaler()

# RobustScaler ì ìš©
data_scaled[robust_columns] = scaler_robust.fit_transform(data[robust_columns])

# ë‚˜ë¨¸ì§€ ë³€ìˆ˜ì—ëŠ” MinMaxScaler ì ìš©
columns_except_robust = [col for col in scale_columns if col not in robust_columns]
data_scaled[columns_except_robust] = scaler_minmax.fit_transform(data[columns_except_robust])
# ê²°ê³¼ ì €ì¥
data_scaled.to_csv('../document/scaled_datalinear.csv', index=False, encoding='utf-8-sig')
print("ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ê°€ 'scaled_data.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[37]:


print("Checking for infinite values...")
print(data[robust_columns].replace([np.inf, -np.inf], np.nan).isnull().sum())


# In[28]:


# ì „ì²˜ë¦¬ ì™„ë£Œ ë°ì´í„° ì €ì¥
data.to_csv('../document/processed_data.csv', index=False, encoding='utf-8-sig')

print("ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°ê°€ 'processed_data.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[41]:


import pandas as pd

# CSV íŒŒì¼ ë¡œë“œ
df = pd.read_csv("../document/scaled_datalinear.csv")  # íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •

# statusê°€ NaNì¸ í–‰ ì œê±°
df_cleaned = df.dropna(subset=["status"])

# ë³€ê²½ëœ ë°ì´í„° í™•ì¸
print(df_cleaned.head())

# ì •ì œëœ ë°ì´í„° ì €ì¥ (ì„ íƒ)
df_cleaned.to_csv("../document/cleaned_datalinear.csv", index=False, encoding='utf-8-sig')


# In[49]:


import pandas as pd

def read_csv_features(csv_file):
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv(csv_file)
    df.columns = df.columns.str.strip()

    # ëª¨ë“  feature(ì»¬ëŸ¼) ì¶œë ¥
    print("CSV íŒŒì¼ì— í¬í•¨ëœ feature ëª©ë¡:")
    print(df.columns.tolist())

    # ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
    print("\në°ì´í„° ìƒ˜í”Œ:")
    print(df.head())

    return df.columns.tolist(), df

# ì‚¬ìš© ì˜ˆì‹œ
csv_file_path = "../document/cleaned_datalinear.csv"  # ì—¬ê¸°ì— CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.
features, df = read_csv_features(csv_file_path)


# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from geopy.distance import geodesic  # ë‘ ì¢Œí‘œ ì‚¬ì´ ê±°ë¦¬ ê³„ì‚°

def calculate_haversine(lat1, lon1, lat2, lon2):
    """ ìœ„ë„(lat)ì™€ ê²½ë„(lon)ë¥¼ ì´ìš©í•´ ë‘ ì§€ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚° (ë‹¨ìœ„: km) """
    if np.isnan(lat1) or np.isnan(lon1) or np.isnan(lat2) or np.isnan(lon2):
        return np.nan  # NaNì´ ìˆëŠ” ê²½ìš° ê±°ë¦¬ë¥¼ NaNìœ¼ë¡œ ì„¤ì •
    return geodesic((lat1, lon1), (lat2, lon2)).km

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv("../document/cleaned_datalinear.csv")

# datetime ë³€í™˜ ë° ì‹œê°„ ê°„ê²© ê³„ì‚°
data["created_at"] = pd.to_datetime(data["created_at"])
data["time_diff"] = data["created_at"].diff().dt.total_seconds()  # ì‹œê°„ ê°„ê²© (ì´ˆ ë‹¨ìœ„)

# ì´ì „ ìœ„ë„, ê²½ë„ ì¶”ê°€ (ì´ë™ ê±°ë¦¬ ê³„ì‚°ìš©)
data["prev_lat"] = data["lat"].shift(1)
data["prev_lon"] = data["lon"].shift(1)

# ì´ë™ ê±°ë¦¬ ê³„ì‚° (NaN ì²˜ë¦¬ëŠ” ìœ ì§€)
data["distance"] = data.apply(lambda row: calculate_haversine(row["prev_lat"], row["prev_lon"], row["lat"], row["lon"]), axis=1)

# ì‹ í˜¸ ì†Œì‹¤ ì—¬ë¶€ íƒì§€ (5ë¶„ = 300ì´ˆ ì´ìƒ ë°ì´í„°ê°€ ëŠê²¼ìœ¼ë©´ ì‹ í˜¸ ì†Œì‹¤)
data["signal_loss"] = data["time_diff"] > 300

# ì´ìƒì¹˜ íƒì§€ ê¸°ì¤€ (ì˜ˆ: 5km ì´ìƒ ì´ë™í•˜ë©´ ì´ìƒì¹˜)
threshold = 5  # km

# ğŸš¨ ì‹ í˜¸ ì†Œì‹¤(5ë¶„) í›„ ì²« ë²ˆì§¸ ì í”„ëŠ” ì´ìƒì¹˜ì—ì„œ ì œì™¸
data["outlier"] = (data["distance"] > threshold) & (~data["signal_loss"].shift(1).fillna(False))

# plt.figure(figsize=(10, 6))

# # ì •ìƒ ë°ì´í„° (íŒŒë€ìƒ‰)
# plt.scatter(data[data["outlier"] == False]["lon"], 
#             data[data["outlier"] == False]["lat"], 
#             label="Normal Data", alpha=0.5, c="blue", s=10)

# # ì´ìƒì¹˜ (ë¹¨ê°„ìƒ‰)
# plt.scatter(data[data["outlier"] == True]["lon"], 
#             data[data["outlier"] == True]["lat"], 
#             label="Outliers", alpha=0.8, c="red", s=30)

# plt.xlabel("Longitude (ê²½ë„)")
# plt.ylabel("Latitude (ìœ„ë„)")
# plt.title("Scatter Plot of GPS Data with Outliers (5-min Signal Loss Considered)")
# plt.legend()
# plt.show()





# In[44]:


# ì´ìƒì¹˜ ì œê±° (ë³´ê°„ ì—†ì´ NaN ìœ ì§€)
data_cleaned = data[data["outlier"] == False].drop(columns=["prev_lat", "prev_lon", "distance", "outlier", "signal_loss", "time_diff"])

# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
data_cleaned.to_csv("../document/cleaned_data_no_outlierslinear.csv", index=False, encoding='utf-8-sig')

print(f"ì´ìƒì¹˜ ì œê±° ì „ ë°ì´í„° í¬ê¸°: {len(data)}")
print(f"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(data_cleaned)}")


# In[45]:


import numpy as np
import pandas as pd

# ë°ì´í„° ë¡œë“œ
file_path = "../document/cleaned_data_no_outlierslinear.csv"  # íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•˜ì„¸ìš”
data = pd.read_csv(file_path)

# datetime ë³€í™˜
data["created_at"] = pd.to_datetime(data["created_at"])

# ì´ì „ ìœ„ë„, ê²½ë„ ì¶”ê°€ (ì´ì „ ì¢Œí‘œì™€ ë¹„êµ)
data["prev_lat"] = data["lat"].shift(1)
data["prev_lon"] = data["lon"].shift(1)

# ê²½ë„ ë° ìœ„ë„ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
data["lat_diff"] = abs(data["lat"] - data["prev_lat"])
data["lon_diff"] = abs(data["lon"] - data["prev_lon"])

# ì‹ í˜¸ ì†Œì‹¤ ì—¬ë¶€ íƒì§€ (5ë¶„ = 300ì´ˆ ì´ìƒ ë°ì´í„°ê°€ ëŠê²¼ìœ¼ë©´ ì‹ í˜¸ ì†Œì‹¤)
data["time_diff"] = data["created_at"].diff().dt.total_seconds()
data["signal_loss"] = data["time_diff"] > 300

# ì‹ í˜¸ ì†Œì‹¤ì´ ì•„ë‹Œ ê²½ìš°, ê²½ë„ ë˜ëŠ” ìœ„ë„ê°€ 0.1 ì´ìƒ ì°¨ì´ë‚˜ëŠ” ë°ì´í„° ì°¾ê¸° (ì´ìƒì¹˜)
outliers = data[(data["signal_loss"] == False) & ((data["lat_diff"] > 0.1) | (data["lon_diff"] > 0.1))]

# ì´ìƒì¹˜ë¥¼ ì œê±°í•œ ë°ì´í„° ìƒì„±
filtered_data = data.drop(outliers.index)

# ì •ë ¬ (MMSI ë° ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬)
filtered_data = filtered_data.sort_values(by=["mmsi", "created_at"])

# ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° ì €ì¥
output_file = "../document/cleaned_data_without_lat_lon_outlierslinear.csv"
filtered_data.to_csv(output_file, index=False, encoding="utf-8-sig")

# ê²°ê³¼ ì¶œë ¥
print(f"ì´ìƒì¹˜ ì œê±°ëœ ë°ì´í„° ê°œìˆ˜: {len(filtered_data)}")
print(f"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„°ê°€ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# In[29]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ
data = pd.read_csv('../document/processed_data.csv')

# ìœ„ë„ì™€ ê²½ë„ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
lat_mean, lat_std = data['lat'].mean(), data['lat'].std()
lon_mean, lon_std = data['lon'].mean(), data['lon'].std()

# ì´ìƒì¹˜ ê¸°ì¤€: í‰ê·  Â± 2 * í‘œì¤€í¸ì°¨
lat_lower, lat_upper = lat_mean - 2 * lat_std, lat_mean + 2 * lat_std
lon_lower, lon_upper = lon_mean - 2 * lon_std, lon_mean + 2 * lon_std

# ì´ìƒì¹˜ ë°ì´í„° íƒì§€
outliers = data[
    (data['lat'] < lat_lower) | (data['lat'] > lat_upper) |
    (data['lon'] < lon_lower) | (data['lon'] > lon_upper)
]

# ì´ìƒì¹˜ ë²”ìœ„ ì¶œë ¥
print("ìœ„ë„ ì´ìƒì¹˜ ë²”ìœ„:")
print(f"Lower: {lat_lower:.4f}, Upper: {lat_upper:.4f}")
print("ê²½ë„ ì´ìƒì¹˜ ë²”ìœ„:")
print(f"Lower: {lon_lower:.4f}, Upper: {lon_upper:.4f}")

# ì´ìƒì¹˜ì™€ ì •ìƒ ë°ì´í„° ë¶„í¬ ì‹œê°í™”
plt.figure(figsize=(14, 6))

# ìœ„ë„ ë¶„í¬
plt.subplot(1, 2, 1)
sns.histplot(data['lat'], kde=True, color='blue', label='Lat Data', bins=30)
plt.axvline(lat_lower, color='red', linestyle='--', label='Lat Lower Bound')
plt.axvline(lat_upper, color='green', linestyle='--', label='Lat Upper Bound')
plt.title('Latitude Distribution with Outliers')
plt.legend()

# ê²½ë„ ë¶„í¬
plt.subplot(1, 2, 2)
sns.histplot(data['lon'], kde=True, color='orange', label='Lon Data', bins=30)
plt.axvline(lon_lower, color='red', linestyle='--', label='Lon Lower Bound')
plt.axvline(lon_upper, color='green', linestyle='--', label='Lon Upper Bound')
plt.title('Longitude Distribution with Outliers')
plt.legend()

plt.tight_layout()
plt.show()

# ì´ìƒì¹˜ì˜ ìœ„ê²½ë„ ì‚°í¬ë„
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data['lon'], y=data['lat'], label='Normal Data', alpha=0.6)
sns.scatterplot(x=outliers['lon'], y=outliers['lat'], color='red', label='Outliers')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Scatter Plot of Latitude and Longitude (Outliers Highlighted)')
plt.legend()
plt.show()

