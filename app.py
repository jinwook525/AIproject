from flask import Flask, jsonify
from flask_socketio import SocketIO, emit
import torch
import preprocessing  # ì „ì²˜ë¦¬ ëª¨ë“ˆ
import model  # AI ëª¨ë¸ ëª¨ë“ˆ
import logging
import numpy as np
app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

@app.route("/")
def home():
    return jsonify({"message": "Flask API is running!"})

logging.basicConfig(filename="debug_log.txt", level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… StandardScaler ë¶ˆëŸ¬ì˜¤ê¸°
lat_lon_scaler = preprocessing.get_lat_lon_scaler()

@app.route("/predict", methods=["GET"])
def predict():
    """ ìµœì‹  ë°ì´í„° ë¶ˆëŸ¬ì™€ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ìˆ˜í–‰ """
    processed_data = preprocessing.preprocess()  # ì „ì²˜ë¦¬ ìˆ˜í–‰

    if processed_data is None:
        return jsonify({"error": "No valid data available"}), 400

    predictions = {}
    for mmsi, tensor in processed_data.items():
        #print(f"Processing MMSI {mmsi}, tensor shape: {tensor.shape}")

        # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
        pred = model.predict(tensor)

        # âœ… predê°€ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° numpy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(pred, list):
            pred = np.array(pred)

        # âœ… predê°€ torch.Tensorì´ë©´ ë³€í™˜
        elif isinstance(pred, torch.Tensor):
            pred = pred.detach().cpu().numpy()

        # ğŸš€ lat, lonì„ ì›ë˜ ê°’ìœ¼ë¡œ ë³€í™˜ (ì—­ìŠ¤ì¼€ì¼ë§)
        if pred.ndim == 1:  # âœ… 1ì°¨ì› ë°°ì—´ì¸ ê²½ìš° reshape í•„ìš”
            pred = pred.reshape(1, -1)

        pred[:, 0:2] = lat_lon_scaler.inverse_transform(pred[:, 0:2])  # âœ… ì²« 2ê°œ ì»¬ëŸ¼(lat, lon)ë§Œ ë³€í™˜

        predictions[mmsi] = {
            "prediction": pred.tolist(),
        }

    return jsonify({"predictions": predictions})




@socketio.on("real_time_request")
def handle_realtime_request():
    """ WebSocketì„ í†µí•´ ì‹¤ì‹œê°„ ìš”ì²­ ì²˜ë¦¬ """
    processed_data = preprocessing.preprocess()  # ì „ì²˜ë¦¬ ìˆ˜í–‰

    if processed_data is None:
        emit("real_time_response", {"error": "No valid data available"})
        return

    # ê° MMSIë³„ AI ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = {mmsi: model.predict(tensor) for mmsi, tensor in processed_data.items()}

    emit("real_time_response", {"predictions": predictions})

if __name__ == "__main__":
    socketio.run(app, debug=True, host="0.0.0.0", port=5000)
