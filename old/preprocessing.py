import numpy as np
import pandas as pd
import torch
import mysql.connector
import os
from dotenv import load_dotenv
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# MySQL ì—°ê²° ì„¤ì •
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")

def create_connection():
    """ MySQL ì—°ê²°ì„ ìƒì„± """
    return mysql.connector.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )

def fetch_latest_ocean_data():
    """ MySQLì—ì„œ í•´ì–‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì—¬ëŸ¬ ê°œ) """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        query = """
        SELECT datetime, wind_speed, air_temperature, humidity, 
               air_pressure, water_temperature, salinity
        FROM oceandata
        ORDER BY datetime DESC
        """
        cursor.execute(query)
        results = cursor.fetchall()

        cursor.close()
        conn.close()

        if not results:
            return None  # ë°ì´í„° ì—†ìŒ

        return pd.DataFrame(results)  # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜

    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None



def fetch_latest_ais_data():
    """ ê° ì„ ë°•(MMSI)ë³„ ìµœì‹  2ê°œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ ìœ„ì¹˜ + ì´ì „ ìœ„ì¹˜) """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        # ê° ì„ ë°•(MMSI)ë³„ ìµœì‹  2ê°œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        query = """
        WITH RankedAIS AS (
            SELECT *, 
                ROW_NUMBER() OVER (PARTITION BY mmsi ORDER BY created_at DESC) AS rn
            FROM aisdatareal
        )
        SELECT * FROM RankedAIS WHERE rn <= 2;
        """
        cursor.execute(query)
        results = cursor.fetchall()

        cursor.close()
        conn.close()

        if not results:
            return None  # ë°ì´í„° ì—†ìŒ

        # ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results)
        # âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ MMSI í™•ì¸
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ MMSI ëª©ë¡:", df["mmsi"].unique()) 
        # MMSIë³„ë¡œ ìµœì‹  ë°ì´í„° 2ê°œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if df["mmsi"].nunique() == 0 or df.shape[0] < 2:
            return None  # ê° ì„ ë°•ë³„ë¡œ 2ê°œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê±°ë¦¬ ê³„ì‚° ë¶ˆê°€

        return df

    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None


def haversine_vectorized(lat1, lon1, lat2, lon2):
    """ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚° """
    R = 6371  # ì§€êµ¬ ë°˜ê²½ (km)
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1

    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

    return R * c  # km ë‹¨ìœ„ ê±°ë¦¬ ë°˜í™˜

# âœ… StandardScaler ì €ì¥ (lat, lon ì—­ìŠ¤ì¼€ì¼ë§ìš©)
lat_lon_scaler = StandardScaler()

def fit_scalers(original_data):
    """ âœ… ì›ë³¸ ë°ì´í„°(`original_data`)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ StandardScaler í•™ìŠµ """
    global lat_lon_scaler
    lat_lon_scaler.fit(original_data[["lat", "lon"]])

def get_lat_lon_scaler():
    """ âœ… lat, lon ì—­ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ StandardScaler ë°˜í™˜ """
    return lat_lon_scaler

def get_original_lat(mmsi):
    """ íŠ¹ì • MMSIì˜ ì›ë˜ ìœ„ë„(lat)ë¥¼ ë°˜í™˜ """
    df_original = fetch_latest_ais_data()  # âœ… ì›ë˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
    original_row = df_original[df_original["mmsi"] == mmsi]
    return original_row["lat"].values[0] if not original_row.empty else None

def get_original_lon(mmsi):
    """ íŠ¹ì • MMSIì˜ ì›ë˜ ê²½ë„(lon)ë¥¼ ë°˜í™˜ """
    df_original = fetch_latest_ais_data()  # âœ… ì›ë˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
    original_row = df_original[df_original["mmsi"] == mmsi]
    return original_row["lon"].values[0] if not original_row.empty else None

def preprocess():
    """AIS ë°ì´í„°ì™€ í•´ì–‘ ë°ì´í„°ë¥¼ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì „ì²˜ë¦¬"""
    df_ais = fetch_latest_ais_data()  # AIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_ocean = fetch_latest_ocean_data()  # í•´ì–‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    if df_ais is None or df_ocean is None or df_ocean.empty:
        print("No valid data available. Skipping processing.")
        return None

    # `created_at`ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df_ais["created_at"] = pd.to_datetime(df_ais["created_at"])
    df_ocean["datetime"] = pd.to_datetime(df_ocean["datetime"])

    # `created_at`ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ `oceandata` ë§¤ì¹­
    df_ocean["key"] = 1  # ë³‘í•©ì„ ìœ„í•œ ê°€ìƒ í‚¤ ìƒì„±
    df_ais["key"] = 1

    merged_df = df_ais.merge(df_ocean, on="key", how="left").drop(columns=["key"])

    # ìƒˆë¡œìš´ `ocean_time_diff` ë³€ìˆ˜ ìƒì„±í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë§Œ ìœ ì§€
    merged_df["ocean_time_diff"] = abs(merged_df["created_at"] - merged_df["datetime"])
    merged_df = merged_df.sort_values(["mmsi", "ocean_time_diff"]).drop_duplicates(subset=["mmsi"], keep="first")

    # `datetime` ì»¬ëŸ¼ ì‚­ì œ (ë” ì´ìƒ í•„ìš” ì—†ìŒ)
    merged_df = merged_df.drop(columns=["datetime", "ocean_time_diff"])

    # ë³‘í•©ëœ ë°ì´í„° ì‚¬ìš©
    df_ais = merged_df.copy()  # ì´í›„ ëª¨ë“  ì²˜ë¦¬ë¥¼ `df_ais`ì—ì„œ ìˆ˜í–‰
    
    # âœ… `lat_lon_scaler` í•™ìŠµ (lat, lon ë³€í™˜ì„ ìœ„í•´ í•„ìˆ˜)
    fit_scalers(df_ais)
    
    #  created_atì´ NaTì¸ì§€ í™•ì¸
    if df_ais["created_at"].isna().sum() > 0:
        print("âš ï¸ Warning: NaT values detected in created_at. Filling with previous values.")
        df_ais["created_at"].fillna(method="ffill", inplace=True)  # ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    
    #  `lat`, `lon`ì´ NaNì¸ í–‰ì„ ì‚­ì œ
    df_ais = df_ais.dropna(subset=["lat", "lon"])
    
    #  `turn`, `speed`, `course`, `heading` NaN ê°’ ë³´ê°„ (ì•/ë’¤ ê°’ìœ¼ë¡œ ì±„ì›€)
    df_ais["turn"] = df_ais["turn"].fillna(method="ffill").fillna(method="bfill")
    df_ais["speed"] = df_ais["speed"].fillna(method="ffill").fillna(method="bfill")
    df_ais["course"] = df_ais["course"].fillna(method="ffill").fillna(method="bfill")
    df_ais["heading"] = df_ais["heading"].fillna(method="ffill").fillna(method="bfill")
    #  ì‹œê°„ ì°¨ì´ ê³„ì‚°
    df_ais = df_ais.sort_values(["mmsi", "created_at"])  # MMSIë³„ ì •ë ¬
    df_ais["time_diff"] = df_ais.groupby("mmsi")["created_at"].diff().dt.total_seconds()

    #  NaNì„ 0ìœ¼ë¡œ ë³€í™˜
    df_ais["time_diff"].fillna(0, inplace=True)

    #  ìµœì¢… NaN ê°œìˆ˜ í™•ì¸
    print("NaN values in time_diff after fillna:", df_ais["time_diff"].isna().sum())

    # ì‹ í˜¸ ì†Œì‹¤ ì—¬ë¶€ (5ë¶„(300ì´ˆ) ì´ìƒ ì‹ í˜¸ ì—†ìŒ)
    df_ais["signal_loss"] = df_ais["time_diff"] > 300

    # ì´ë™ ê±°ë¦¬ ê³„ì‚°
    df_ais["prev_lat"] = df_ais.groupby("mmsi")["lat"].shift(1)
    df_ais["prev_lon"] = df_ais.groupby("mmsi")["lon"].shift(1)

    df_ais["distance"] = df_ais.apply(lambda row: haversine_vectorized(row["prev_lat"], row["prev_lon"], row["lat"], row["lon"]) 
                                      if not pd.isna(row["prev_lat"]) else np.nan, axis=1)

    # ì´ë™ ê±°ë¦¬ ì´ìƒì¹˜ í•„í„°ë§ (5km ì´ìƒ ì´ë™í•œ ê²½ìš° + ì‹ í˜¸ ì†Œì‹¤ ì—†ìœ¼ë©´ ì œê±°)
    df_ais = df_ais[~((df_ais["distance"] > 5) & (~df_ais["signal_loss"]))]

    # ìœ„ë„(lat) ë˜ëŠ” ê²½ë„(lon)ê°€ 0.1 ì´ìƒ ì°¨ì´ë‚˜ëŠ” ì´ìƒì¹˜ ë°ì´í„° ì œê±°
    df_ais["lat_diff"] = abs(df_ais["lat"] - df_ais["prev_lat"])
    df_ais["lon_diff"] = abs(df_ais["lon"] - df_ais["prev_lon"])

    # ì´ìƒì¹˜ ì¡°ê±´: (ì‹ í˜¸ ì†Œì‹¤ ì—†ìŒ) + (lat/lon ë³€í™”ëŸ‰ 0.1 ì´ìƒ) + (ì´ë™ ê±°ë¦¬ 5km ë¯¸ë§Œ)
    df_ais = df_ais[~(((df_ais["lat_diff"] > 0.1) | (df_ais["lon_diff"] > 0.1)) & (df_ais["distance"] < 5) & (~df_ais["signal_loss"]))]

    # í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ
    df_ais = df_ais.drop(columns=["prev_lat", "prev_lon", "signal_loss", "lat_diff", "lon_diff","distance"])

    # status ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰
    df_ais["status"] = df_ais["status"].fillna("nan")  # NaN ê°’ì„ 'nan' ë¬¸ìì—´ë¡œ ë³€í™˜
    status_encoded = pd.get_dummies(df_ais["status"], prefix="status")  # ì›-í•« ì¸ì½”ë”©
    status_encoded = status_encoded.astype(int)  # True/Falseë¥¼ 0/1ë¡œ ë³€í™˜

    # ëª¨ë“  ê°€ëŠ¥í•œ status ì»¬ëŸ¼ì„ ê°•ì œë¡œ í¬í•¨ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì¶”ê°€)
    expected_status_columns = [
        "status_0.0", "status_1.0", "status_2.0", "status_3.0", "status_5.0",
        "status_6.0", "status_7.0", "status_8.0", "status_9.0", "status_10.0",
        "status_11.0", "status_12.0", "status_15.0"
    ]
    for col in expected_status_columns:
        if col not in status_encoded:
            status_encoded[col] = 0  # ì—†ëŠ” ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ìš°ê¸°

    # ì›ë³¸ ë°ì´í„°ì™€ ì›-í•« ì¸ì½”ë”©ëœ ë°ì´í„° ë³‘í•©
    df_ais = pd.concat([df_ais, status_encoded], axis=1)
    df_ais = df_ais.drop(columns=["status"])  # ì›ë³¸ 'status' ì»¬ëŸ¼ ì‚­ì œ

    # ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” 30ê°œ ì…ë ¥ ë³€ìˆ˜ë§Œ ì„ íƒ
    selected_columns = [
        "turn", "speed", "accuracy", "course", "heading", "lat", "lon",
        "time_diff", "wind_direct","wind_speed", "surface_curr_drc","surface_curr_speed","air_temperature", 
        "air_pressure", "water_temperature","humidity", "salinity"
    ] + expected_status_columns  # ê°•ì œë¡œ í¬í•¨ëœ status ì»¬ëŸ¼ ì¶”ê°€
    # ğŸš€ `df_ais`ì— NaN ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
    print("NaN values per column before scaling:\n", df_ais.isna().sum())

    # ğŸš€ NaN ê°’ì´ í¬í•¨ëœ ê²½ìš° 0ìœ¼ë¡œ ë³€í™˜
    if df_ais.isna().sum().sum() > 0:
        print("âš ï¸ Warning: NaN values detected in df_ais before scaling. Replacing NaN with 0.")
        df_ais.fillna(0, inplace=True)

    # ğŸš€ df_aisì—ì„œ ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
    print("Available columns in df_ais:", df_ais.columns.tolist())

    missing_columns = [col for col in selected_columns if col not in df_ais.columns]
    if missing_columns:
        print(f"Missing columns: {missing_columns}")

    # ğŸš€ ì„ íƒëœ ì»¬ëŸ¼ì´ ëˆ„ë½ëœ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€
    for col in selected_columns:
        if col not in df_ais.columns:
            df_ais[col] = 0  # ëˆ„ë½ëœ ì»¬ëŸ¼ì„ 0ìœ¼ë¡œ ì¶”ê°€

    # ğŸš€ ì´ì œ KeyError ë°œìƒ ì—†ì´ ì„ íƒëœ ì»¬ëŸ¼ë§Œ ìœ ì§€
    df_ais_mmsi = df_ais[["mmsi"]]
    df_ais = df_ais[selected_columns]

    # ğŸš€ ë°ì´í„° í¬ê¸° í™•ì¸
    print(f"Final df_ais shape: {df_ais.shape}")
    
    

    # âœ… ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¶„ë¥˜
    robust_columns = ["turn"]  # ì´ìƒì¹˜ì— ê°•í•œ RobustScaler ì ìš©
    standard_columns = ["lat", "lon"]  # StandardScaler ì ìš©
    minmax_columns = [
        "speed", "accuracy", "course", "heading", "time_diff",
        "wind_speed", "air_temperature", "humidity", "air_pressure",
        "water_temperature", "salinity"
    ]  # MinMaxScaler ì ìš©

    # âœ… status_* ì»¬ëŸ¼ì„ ì œì™¸ (ì›-í•« ì¸ì½”ë”©ëœ ê°’ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”)
    status_columns = [col for col in df_ais.columns if col.startswith("status_")]

    # âœ… ColumnTransformer êµ¬ì„± (status_* ì»¬ëŸ¼ ì œì™¸)
    scaler = ColumnTransformer([
        ("robust", RobustScaler(), robust_columns),
        ("standard", StandardScaler(), standard_columns),
        ("minmax", MinMaxScaler(), minmax_columns)
    ], remainder="passthrough")  # â— status_* ì»¬ëŸ¼ì„ ë³€ê²½ ì—†ì´ ìœ ì§€

    # âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    normalized_data = scaler.fit_transform(df_ais)

    # âœ… ë°ì´í„° í¬ê¸° í™•ì¸
    print("Shape of scaled data:", normalized_data.shape)   


    # PyTorch Tensor ë³€í™˜ (ê° MMSIë³„ ë°ì´í„° ë°˜í™˜)
    tensors = {mmsi: torch.tensor(row, dtype=torch.float32)
              for mmsi, row in zip(df_ais_mmsi["mmsi"], normalized_data)}

    print('preprocessing end')
    return tensors  # MMSIë³„ í…ì„œ ë°˜í™˜




    

if __name__ == "__main__":
    # ğŸ”¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    processed_data = preprocess()
    print("ì‹¤ì‹œê°„ ì „ì²˜ë¦¬ëœ ë°ì´í„°:", processed_data)

