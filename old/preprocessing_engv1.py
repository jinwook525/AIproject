import numpy as np
import pandas as pd
import mysql.connector
import os
import torch
from dotenv import load_dotenv
from geopy.distance import geodesic
from sklearn.preprocessing import MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import RobustScaler

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# MySQL ì—°ê²° ì„¤ì •
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")

def create_connection():
    """ MySQL ì—°ê²° ìƒì„± """
    return mysql.connector.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )

def fetch_latest_ais_data():
    """ ê° ì„ ë°•(MMSI)ë³„ ìµœì‹  2ê°œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        query = """
        WITH RankedAIS AS (
            SELECT *, 
                ROW_NUMBER() OVER (PARTITION BY mmsi ORDER BY created_at DESC) AS rn
            FROM aisdatareal
        )
        SELECT * FROM RankedAIS WHERE rn <= 2;
        """
        cursor.execute(query)
        results = cursor.fetchall()

        cursor.close()
        conn.close()

        if not results:
            return None  # ë°ì´í„° ì—†ìŒ

        df = pd.DataFrame(results)
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ MMSI ëª©ë¡:", df["mmsi"].unique())  

        if df["mmsi"].nunique() == 0 or df.shape[0] < 2:
            return None  # MMSIë³„ ìµœì‹  2ê°œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê±°ë¦¬ ê³„ì‚° ë¶ˆê°€

        return df

    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None

def fetch_latest_ocean_data():
    """ MySQLì—ì„œ ìµœì‹  í•´ì–‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° """
    try:
        conn = create_connection()
        cursor = conn.cursor(dictionary=True)

        query = """
        SELECT datetime, wind_speed, air_temperature, humidity, 
               air_pressure, water_temperature, salinity
        FROM oceandata
        ORDER BY datetime DESC
        """
        cursor.execute(query)
        results = cursor.fetchall()

        cursor.close()
        conn.close()

        if not results:
            return None  # ë°ì´í„° ì—†ìŒ

        return pd.DataFrame(results)  # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜

    except mysql.connector.Error as e:
        print(f"MySQL error: {e}")
        return None

def preprocess_ais_data():
    """ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¶ˆëŸ¬ì˜¨ AIS ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ """
    df_ais = fetch_latest_ais_data()
    df_ocean = fetch_latest_ocean_data()

    if df_ais is None or df_ocean is None or df_ocean.empty:
        print("No valid ocean data available. Skipping processing.")
        return None

    # ì‹œê°„ ë°ì´í„° ë³€í™˜
    df_ais["created_at"] = pd.to_datetime(df_ais["created_at"])
    df_ocean["datetime"] = pd.to_datetime(df_ocean["datetime"])

    # âœ… ëŒ€í•œë¯¼êµ­ ê²½ê³„ ë‚´ ë°ì´í„° í•„í„°ë§
    lat_min, lat_max = 34.5, 35.5
    lon_min, lon_max = 128.5, 130.0
    df_ais = df_ais[(df_ais['lat'] >= lat_min) & (df_ais['lat'] <= lat_max) &
                    (df_ais['lon'] >= lon_min) & (df_ais['lon'] <= lon_max)]
    print(f"ğŸ”¹ ëŒ€í•œë¯¼êµ­ ê²½ê³„ ë‚´ ë°ì´í„° í¬ê¸°: {df_ais.shape}")

    # âœ… í•´ì–‘ ë°ì´í„° ë³‘í•©
    df_ocean["key"] = 1  
    df_ais["key"] = 1
    merged_df = df_ais.merge(df_ocean, on="key", how="left").drop(columns=["key"])
    merged_df["ocean_time_diff"] = abs(merged_df["created_at"] - merged_df["datetime"])
    merged_df = merged_df.sort_values(["mmsi", "ocean_time_diff"]).drop_duplicates(subset=["mmsi"], keep="first")
    merged_df = merged_df.drop(columns=["datetime", "ocean_time_diff"])
    df_ais = merged_df.copy()

    # âœ… `salinity` ê°’ ì²˜ë¦¬ (ë¬¸ì â†’ ìˆ«ìë¡œ ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬)
    df_ais["salinity"] = df_ais["salinity"].astype(str)  # ë¬¸ìì—´ ë³€í™˜
    df_ais["salinity"] = df_ais["salinity"].replace("-", np.nan)  # '-' ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
    df_ais["salinity"] = df_ais["salinity"].apply(lambda x: x if x.replace('.', '', 1).isdigit() else np.nan)  # ìˆ«ìê°€ ì•„ë‹Œ ê°’ NaN ì²˜ë¦¬
    df_ais["salinity"] = df_ais["salinity"].astype(float)  # float íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df_ais["salinity"].fillna(method="ffill", inplace=True)  # NaN ê°’ ì•ì˜ ê°’ìœ¼ë¡œ ì±„ì›€
    df_ais["salinity"].fillna(method="bfill", inplace=True)  # ê·¸ë˜ë„ NaNì´ë©´ ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ì›€

    # âœ… ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš© (lat, lon)
    scaler = MinMaxScaler(feature_range=(1, 10))
    df_ais[["lat", "lon"]] = scaler.fit_transform(df_ais[["lat", "lon"]])
    df_ais[["lat", "lon"]] = np.log1p(df_ais[["lat", "lon"]])

    # âœ… `time_diff` ê³„ì‚° ë° NaN ì²˜ë¦¬
    df_ais["time_diff"] = df_ais.groupby("mmsi")["created_at"].diff().dt.total_seconds()
    df_ais["time_diff"].fillna(0, inplace=True)

    # âœ… ì‹ í˜¸ ì†Œì‹¤ ì—¬ë¶€ ì¶”ê°€
    df_ais["signal_loss"] = df_ais["time_diff"] > 300

    # âœ… `turn`, `speed`, `course`, `heading` ê²°ì¸¡ì¹˜ ë³´ê°„
    for col in ["turn", "speed", "course", "heading"]:
        df_ais[col] = df_ais[col].fillna(method="ffill").fillna(method="bfill")

    # âœ… Feature Engineering (ì¶”ê°€ ë³€ìˆ˜ ìƒì„±)
    df_ais["acceleration"] = (df_ais["speed"] - df_ais.groupby("mmsi")["speed"].shift(1)) / df_ais["time_diff"]
    df_ais["heading_diff"] = (df_ais["heading"] - df_ais.groupby("mmsi")["heading"].shift(1) + 180) % 360 - 180

    for window in [5, 10, 30]:  
        df_ais[f"avg_speed_{window}steps"] = df_ais.groupby("mmsi")["speed"].rolling(window).mean().reset_index(level=0, drop=True)
        df_ais[f"avg_heading_{window}steps"] = df_ais.groupby("mmsi")["heading"].rolling(window).mean().reset_index(level=0, drop=True)

    # âœ… ì‹ í˜¸ ì†Œì‹¤ ë° ê±°ë¦¬ ì´ìƒì¹˜ ì œê±°
    df_ais["prev_lat"] = df_ais.groupby("mmsi")["lat"].shift(1)
    df_ais["prev_lon"] = df_ais.groupby("mmsi")["lon"].shift(1)
    df_ais["distance_km"] = df_ais.apply(lambda row: geodesic((row["prev_lat"], row["prev_lon"]), (row["lat"], row["lon"])).km if pd.notnull(row["prev_lat"]) else 0, axis=1)
    df_ais = df_ais[~((df_ais["distance_km"] > 5) & (~df_ais["signal_loss"]))]
 # âœ… ìŠ¤ì¼€ì¼ë§ ì ìš©
    scale_columns = [
        'turn', 'speed', 'accuracy', 'course', 'heading', 
        'wind_direct', 'wind_speed', 'surface_curr_drc', 'surface_curr_speed', 
        'air_temperature', 'water_temperature', 'air_pressure', 'humidity', 'salinity',
        'acceleration', 'heading_diff', 'distance_km', 'avg_speed_5steps', 'avg_heading_5steps',
        'position_change_5steps', 'avg_speed_10steps', 'avg_heading_10steps', 'position_change_10steps', 
        'avg_speed_30steps', 'avg_heading_30steps', 'position_change_30steps'
    ]

    # RobustScaler & MinMaxScaler ì •ì˜
    robust_columns = ['turn', 'acceleration', 'heading_diff']
    minmax_columns = [col for col in scale_columns if col not in robust_columns]

    # NaN ê°’ ì²˜ë¦¬ ë° ì´ìƒì¹˜ ë³€í™˜
    df_ais[robust_columns] = df_ais[robust_columns].replace([np.inf, -np.inf], np.nan)
    for col in robust_columns:
        df_ais[col].fillna(method='ffill', inplace=True)
        df_ais[col].fillna(method='bfill', inplace=True)
        df_ais[col].fillna(0, inplace=True)

    # ColumnTransformer ì ìš©
    scaler = ColumnTransformer([
        ("robust", RobustScaler(), robust_columns),
        ("minmax", MinMaxScaler(), minmax_columns)
    ], remainder="passthrough")

    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaled_data = scaler.fit_transform(df_ais)
    # âœ… PyTorch Tensor ë³€í™˜
    tensors = {mmsi: torch.tensor(row, dtype=torch.float32) for mmsi, row in zip(df_ais["mmsi"], df_ais.values)}

    print("ğŸš€ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    return tensors

if __name__ == "__main__":
    processed_data = preprocess_ais_data()
    print("ğŸš€ ì „ì²˜ë¦¬ëœ ë°ì´í„°:", processed_data)
