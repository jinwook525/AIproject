from flask import Flask, jsonify
import torch
import preprocessing  # ì „ì²˜ë¦¬ ëª¨ë“ˆ
import model  # AI ëª¨ë¸ ëª¨ë“ˆ
import logging
import numpy as nppy

app = Flask(__name__)

@app.route("/")
def home():
    return jsonify({"message": "Flask API is running!"})

logging.basicConfig(filename="debug_log.txt", level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… StandardScaler ë¶ˆëŸ¬ì˜¤ê¸°
lat_lon_scaler = preprocessing.get_lat_lon_scaler()


@app.route("/predict", methods=["GET"])
def predict():
    """ ìµœì‹  ë°ì´í„° ë¶ˆëŸ¬ì™€ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ìˆ˜í–‰ """
    try:
        processed_data = preprocessing.preprocess()  # ì „ì²˜ë¦¬ ìˆ˜í–‰

        if processed_data is None or not processed_data:
            logging.error("âŒ No valid data available for prediction.")
            return jsonify({"error": "No valid data available"}), 400

        print("âœ… Flaskì—ì„œ ë°›ì€ processed_dataì˜ mmsi ëª©ë¡:", list(processed_data.keys()))
        logging.info(f"âœ… MMSI ëª©ë¡: {list(processed_data.keys())}")

        predictions = {}

        for mmsi, tensor in processed_data.items():
            if not isinstance(tensor, torch.Tensor):
                tensor = torch.tensor(tensor, dtype=torch.float32).to(model.device)

            if tensor.dim() == 2:
                tensor = tensor.unsqueeze(0)  # (1, sequence_length, features)

            # âœ… ì…ë ¥ ë°ì´í„° ì°¨ì› í™•ì¸
            logging.info(f"ğŸ“Œ MMSI {mmsi} ì˜ˆì¸¡ ì…ë ¥ shape: {tensor.shape}")

            try:
                pred = model.predict(tensor)
            except Exception as e:
                logging.error(f"âŒ Prediction error for MMSI {mmsi}: {e}")
                return jsonify({"error": f"Prediction failed for MMSI {mmsi}", "details": str(e)}), 500

            if isinstance(pred, list):
                pred = np.array(pred)
            elif isinstance(pred, torch.Tensor):
                pred = pred.detach().cpu().numpy()

            if pred.ndim == 1:
                pred = pred.reshape(1, -1)

            try:
                pred[:, 0:2] = lat_lon_scaler.inverse_transform(pred[:, 0:2])
            except Exception as e:
                logging.error(f"âš ï¸ Scaling error for MMSI {mmsi}: {e}")
                continue  # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ MMSI ë°ì´í„° ê±´ë„ˆë›°ê¸°

            predictions[mmsi] = {
                "mmsi": mmsi,
                "prediction": pred.tolist(),
            }

        return jsonify({"predictions": predictions})

    except Exception as e:
        logging.error(f"ğŸš¨ Internal Server Error: {e}")
        return jsonify({"error": "Internal Server Error"}), 500


if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=5000)
